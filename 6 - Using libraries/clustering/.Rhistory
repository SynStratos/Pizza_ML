install.packages("dbscan")
install.packages("car")
install.packages("cluster")
install.packages("mclust")
install.packages("devtools")
install.packages("mlbench")
install.packages("datasets")
devtools::install_github("kassambara/factoextra")
devtools::install_github("kassambara/factoextra")
if(!require(devtools)) install.packages("devtools", dependencies=TRUE)
devtools::install_github("kassambara/factoextra")
install.packages("factoextra")
rm(list=ls())
rm(list=ls())
data <- read.csv("candy_2.csv", header=FALSE)
colnames(data) <- c("caramel", "peanutyalmondy", "nougat", "crispedricewafer", "hard", "bar", "pluribus", "sugarpercent", "pricepercent", "winpercent", "choco", "fruit", "other")
# funzione per unire tutte le 3 classi in una singola colonna e assegnare un valore diverso per classe
all_classes <- function(y){
c <- rep(1,nrow(y))
for( i in 1:nrow(y)){
if(y[i,1]==1)   c[i] <- 0
if(y[i,2]==1)   c[i] <- 1
if(y[i,3]==1)   c[i] <- 2
}
c
}
multi <- all_classes(data[,11:13])
data <- cbind(data[,1:10], multi)
library(datasets)
library(ggplot2)
library(cluster)
library(factoextra)
rm(list=ls())
data <- read.csv("candy_2.csv", header=FALSE)
colnames(data) <- c("caramel", "peanutyalmondy", "nougat", "crispedricewafer", "hard", "bar", "pluribus", "sugarpercent", "pricepercent", "winpercent", "choco", "fruit", "other")
# funzione per unire tutte le 3 classi in una singola colonna e assegnare un valore diverso per classe
all_classes <- function(y){
c <- rep(1,nrow(y))
for( i in 1:nrow(y)){
if(y[i,1]==1)   c[i] <- 0
if(y[i,2]==1)   c[i] <- 1
if(y[i,3]==1)   c[i] <- 2
}
c
}
multi <- all_classes(data[,11:13])
data <- cbind(data[,1:10], multi)
ggplot(data, aes(caramel, hard, color=multi))
ggplot(data, aes(caramel, hard, color=multi)) + geom_plot()
ggplot(data, aes(caramel, hard, color=multi)) + geom_point()
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
ggplot(data, aes(caramel, hard, color=multi)) + geom_point()
ggplot(data, aes(caramel, sugarpercent, color=multi)) + geom_point()
ggplot(data, aes(crispedricewafer, sugarpercent, color=multi)) + geom_point()
ggplot(data, aes(pricepercent, sugarpercent, color=multi)) + geom_point()
rm(list=ls())
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
# k-means
kmx = kmeans( iris[,3:4], 3, nstart=20)
table(kmx$cluster, iris$Species)
kmx$cluster <- as.factor(kmx$cluster)
ggplot(iris, aes(Petal.Length, Petal.Width, color=kmx$cluster)) + geom_point()
fviz_cluster(object = kmx, data = iris[, 3:4], geom = "point",
# choose.vars = c("Sepal.Length", "Sepal.Width"), stand = FALSE,
ellipse.type = "norm") + theme_bw()
library(datasets)
library(ggplot2)
library(cluster)
library(factoextra)
rm(list=ls())
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
# k-means
kmx = kmeans( iris[,3:4], 3, nstart=20)
table(kmx$cluster, iris$Species)
kmx$cluster <- as.factor(kmx$cluster)
ggplot(iris, aes(Petal.Length, Petal.Width, color=kmx$cluster)) + geom_point()
fviz_cluster(object = kmx, data = iris[, 3:4], geom = "point",
# choose.vars = c("Sepal.Length", "Sepal.Width"), stand = FALSE,
ellipse.type = "norm") + theme_bw()
install.packages
install.packages("factoextra")
install.packages("factoextra", dependencies = T)
library(cluster)
library(factoextra)
rm(list=ls())
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
# k-means
kmx = kmeans( iris[,3:4], 3, nstart=20)
table(kmx$cluster, iris$Species)
kmx$cluster <- as.factor(kmx$cluster)
ggplot(iris, aes(Petal.Length, Petal.Width, color=kmx$cluster)) + geom_point()
fviz_cluster(object = kmx, data = iris[, 3:4], geom = "point",
# choose.vars = c("Sepal.Length", "Sepal.Width"), stand = FALSE,
ellipse.type = "norm") + theme_bw()
library(factoextra)
.libPaths()
install.packages("FactoMineR")
install.packages("car")
install.packages("rio")
install.packages("curl")
install.packages("libcurl")
install.packages("curl")
install.packages("curl")
install.packages("factoextra", dependencies=T)
library(datasets)
library(ggplot2)
library(cluster)
library(factoextra)
rm(list=ls())
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
# k-means
kmx = kmeans( iris[,3:4], 3, nstart=20)
table(kmx$cluster, iris$Species)
kmx$cluster <- as.factor(kmx$cluster)
ggplot(iris, aes(Petal.Length, Petal.Width, color=kmx$cluster)) + geom_point()
fviz_cluster(object = kmx, data = iris[, 3:4], geom = "point",
# choose.vars = c("Sepal.Length", "Sepal.Width"), stand = FALSE,
ellipse.type = "norm") + theme_bw()
library(dbscan)
# k-medoids
pamx <- pam(iris[,3:4], 3)
librar(mclust)
library(mclust)
pamx$cluster <- as.factor(pamx$cluster)
ggplot(iris, aes(Petal.Length, Petal.Width, color = pamx$cluster)) + geom_point()
fviz_cluster(pamx, geom = "point",
ellipse.type = "norm") + theme_bw()
kmx$cluster <- as.factor(kmx$cluster)
ggplot(iris, aes(Petal.Length, Petal.Width, color=kmx$cluster)) + geom_point()
fviz_cluster(object = kmx, data = iris[, 3:4], geom = "point",
# choose.vars = c("Sepal.Length", "Sepal.Width"), stand = FALSE,
ellipse.type = "norm") + theme_bw()
# k-medoids
pamx <- pam(iris[,3:4], 3)
pamx$cluster <- as.factor(pamx$cluster)
ggplot(iris, aes(Petal.Length, Petal.Width, color = pamx$cluster)) + geom_point()
fviz_cluster(pamx, geom = "point",
ellipse.type = "norm") + theme_bw()
help("kmeans")
help("fviz_cluster")
help("pam")
help("Mclust")
#---------- GAUSSIAN MIXTURE MODELS ---------#
# Model-based clustering based on parameterized finite Gaussian mixture models.
# G == number of clusters
gmm <- Mclust(iris[,3:4], G = 3, modelNames = c("VVE"))
plot(gmm, what = "classification")
fviz_cluster(object = gmm, data = iris, geom = "point",
ellipse.type = "norm") + theme_bw()
help("mclustBIC")
# BIC for parameterized Gaussian mixture models fitted by EM algorithm initialized by model-based hierarchical clustering.
bic <- mclustBIC(iris[,3:4])
gmm <- Mclust(iris[,3:4], x = bic)
plot(gmm, what = "classification")
fviz_cluster(object = gmm, data = iris, geom = "point",
ellipse.type = "norm") + theme_bw()
install.packages("fpc")
kNNdistplot(iris[, 3:4], k=4)
help("kNNdistplot")
abline(h=0.2,lty=2)
dbm = fpc:dbscan(iris[, 3:4], eps=0.2, MinPts=4)
library(fpc)
library(datasets)
library(ggplot2)
library(cluster)
library(factoextra)
library(dbscan)
library(mclust)
library(fpc)
rm(list=ls())
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
#---------- K-MEANS ---------#
# Perform k-means clustering on a data matrix.
kmx = kmeans( iris[,3:4], 3, nstart=20)
table(kmx$cluster, iris$Species)
kmx$cluster <- as.factor(kmx$cluster)
ggplot(iris, aes(Petal.Length, Petal.Width, color=kmx$cluster)) + geom_point()
# Provides ggplot2-based elegant visualization of partitioning methods
fviz_cluster(object = kmx, data = iris[, 3:4], geom = "point",
# choose.vars = c("Sepal.Length", "Sepal.Width"), stand = FALSE,
ellipse.type = "norm") + theme_bw()
#---------- K-MEDOIDS ---------#
# Partitioning (clustering) of the data into k clusters “around medoids”, a more robust version of K-means.
pamx <- pam(iris[,3:4], 3)
pamx$cluster <- as.factor(pamx$cluster)
ggplot(iris, aes(Petal.Length, Petal.Width, color = pamx$cluster)) + geom_point()
fviz_cluster(pamx, geom = "point",
ellipse.type = "norm") + theme_bw()
#---------- GAUSSIAN MIXTURE MODELS ---------#
# Model-based clustering based on parameterized finite Gaussian mixture models.
# G == number of clusters
gmm <- Mclust(iris[,3:4], G = 3, modelNames = c("VVE"))
plot(gmm, what = "classification")
fviz_cluster(object = gmm, data = iris, geom = "point",
ellipse.type = "norm") + theme_bw()
# BIC for parameterized Gaussian mixture models fitted by EM algorithm initialized by model-based hierarchical clustering.
bic <- mclustBIC(iris[,3:4])
gmm <- Mclust(iris[,3:4], x = bic)
plot(gmm, what = "classification")
fviz_cluster(object = gmm, data = iris, geom = "point",
ellipse.type = "norm") + theme_bw()
#---------- DBSCAN ---------#
# Fast calculation of the k-nearest neighbor distances in a matrix of points.
kNNdistplot(iris[, 3:4], k=4) # k == number of nearest neighbors
abline(h=0.2,lty=2)
dbm = fpc:dbscan(iris[, 3:4], eps=0.2, MinPts=4)
fviz_cluster(dbm, iris[, 3:4], geom = "point")
dbm = fpc::dbscan(iris[, 3:4], eps=0.2, MinPts=4)
fviz_cluster(dbm, iris[, 3:4], geom = "point")
help("hdbscan")
#---------- HDBSCAN ---------#
# Fast implementation of the HDBSCAN (Hierarchical DBSCAN) and its related algorithms using Rcpp.
hdbm = hdbscan(iris[, 3:4], minPts=4)
plot(iris[, 3:4], col=hdbm$cluster+1, pch=20)
help("boxplot")
#---------- BOXPLOT OUTLIERS ----------#
# Produce box-and-whisker plot(s) of the given (grouped) values.
res <- boxplot(iris, outline=F)
#---------- BOXPLOT OUTLIERS ----------#
# Produce box-and-whisker plot(s) of the given (grouped) values.
res <- boxplot(iris[,3:4], outline=F)
#---------- BOXPLOT OUTLIERS ----------#
# Produce box-and-whisker plot(s) of the given (grouped) values.
res <- boxplot(iris[,3:4], outline=F)
#---------- HDBSCAN ---------#
# Fast implementation of the HDBSCAN (Hierarchical DBSCAN) and its related algorithms using Rcpp.
hdbm = hdbscan(iris[, 3:4], minPts=4)
plot(iris[, 3:4], col=hdbm$cluster+2, pch=20) # vengono solo due cluster ?????
#---------- HDBSCAN ---------#
# Fast implementation of the HDBSCAN (Hierarchical DBSCAN) and its related algorithms using Rcpp.
hdbm = hdbscan(iris[, 3:4], minPts=4)
library(datasets)
library(ggplot2)
library(cluster)
library(factoextra)
library(dbscan)
library(mclust)
library(fpc)
rm(list=ls())
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
ntation of the HDBSCAN (Hierarchical DBSCAN) and its related algorithms using Rcpp.
hdbm = hdbscan
#---------- HDBSCAN ---------#
# Fast implementation of the HDBSCAN (Hierarchical DBSCAN) and its related algorithms using Rcpp.
hdbm = hdbscan(iris[, 3:4], minPts=4)
plot(iris[, 3:4], col=hdbm$cluster+2, pch=20) # vengono solo due cluster ?????
plot(iris[, 3:4], col=hdbm$cluster, pch=20) # vengono solo due cluster ?????
plot(iris[, 3:4], col=hdbm$cluster+1, pch=20) # vengono solo due cluster ?????
plot(iris[, 3:4], col=hdbm$cluster+1, pch=25) # vengono solo due cluster ?????
#---------- HDBSCAN ---------#
# Fast implementation of the HDBSCAN (Hierarchical DBSCAN) and its related algorithms using Rcpp.
hdbm = hdbscan(iris[, 3:4], minPts=4)
plot(iris[, 3:4], col=hdbm$cluster+1, pch=20) # vengono solo due cluster ?????
help("hdbscan")
plot(hdbm)
plot(iris[, 3:4], col=hdbm$cluster+1, pch=20) # vengono solo due cluster ?????
#---------- HDBSCAN ---------#
# Fast implementation of the HDBSCAN (Hierarchical DBSCAN) and its related algorithms using Rcpp.
hdbm = hdbscan(iris[, 3:4], minPts=8)
plot(iris[, 3:4], col=hdbm$cluster+1, pch=20) # vengono solo due cluster ?????
#---------- HDBSCAN ---------#
# Fast implementation of the HDBSCAN (Hierarchical DBSCAN) and its related algorithms using Rcpp.
hdbm = hdbscan(iris[, 3:4], minPts=5)
plot(iris[, 3:4], col=hdbm$cluster+1, pch=20) # vengono solo due cluster ?????
#---------- HDBSCAN ---------#
# Fast implementation of the HDBSCAN (Hierarchical DBSCAN) and its related algorithms using Rcpp.
hdbm = hdbscan(iris[, 3:4], minPts=3)
plot(iris[, 3:4], col=hdbm$cluster+1, pch=20) # vengono solo due cluster ?????
#---------- BOXPLOT OUTLIERS ----------#
# Produce box-and-whisker plot(s) of the given (grouped) values.
res <- boxplot(iris[,3:4], outline=F)
#---------- HDBSCAN ---------#
# Fast implementation of the HDBSCAN (Hierarchical DBSCAN) and its related algorithms using Rcpp.
hdbm = hdbscan(iris[, 3:4], minPts=2)
plot(iris[, 3:4], col=hdbm$cluster+1, pch=20) # vengono solo due cluster ?????
print(hdbm$cluster)
#---------- HDBSCAN ---------#
# Fast implementation of the HDBSCAN (Hierarchical DBSCAN) and its related algorithms using Rcpp.
hdbm = hdbscan(iris[, 3:4], minPts=2,     gen_simplified_tree = T)
plot(iris[, 3:4], col=hdbm$cluster+1, pch=20) # vengono solo due cluster ?????
plot(hdbm$simplified_tree)
#---------- HDBSCAN ---------#
# Fast implementation of the HDBSCAN (Hierarchical DBSCAN) and its related algorithms using Rcpp.
hdbm = hdbscan(iris[, 3:4], minPts=2,     gen_simplified_tree = T, xdist=2)
plot(iris[, 3:4], col=hdbm$cluster+1, pch=20) # vengono solo due cluster ?????
#---------- HDBSCAN ---------#
# Fast implementation of the HDBSCAN (Hierarchical DBSCAN) and its related algorithms using Rcpp.
hdbm = hdbscan(iris[, 3:4], minPts=2,     gen_simplified_tree = T, xdist=0.1)
#---------- HDBSCAN ---------#
# Fast implementation of the HDBSCAN (Hierarchical DBSCAN) and its related algorithms using Rcpp.
hdbm = hdbscan(iris[, 3:4], minPts=20,     gen_simplified_tree = T)
plot(iris[, 3:4], col=hdbm$cluster+1, pch=20) # vengono solo due cluster ?????
#---------- HDBSCAN ---------#
# Fast implementation of the HDBSCAN (Hierarchical DBSCAN) and its related algorithms using Rcpp.
hdbm = hdbscan(iris[, 3:4], minPts=40,     gen_simplified_tree = T)
plot(iris[, 3:4], col=hdbm$cluster+1, pch=20) # vengono solo due cluster ?????
#---------- HDBSCAN ---------#
# Fast implementation of the HDBSCAN (Hierarchical DBSCAN) and its related algorithms using Rcpp.
hdbm = hdbscan(iris[, 3:4], minPts=100,     gen_simplified_tree = T)
plot(iris[, 3:4], col=hdbm$cluster+1, pch=20) # vengono solo due cluster ?????
#---------- HDBSCAN ---------#
# Fast implementation of the HDBSCAN (Hierarchical DBSCAN) and its related algorithms using Rcpp.
hdbm = hdbscan(iris[, 3:4], minPts=100,     gen_simplified_tree = T)
plot(iris[, 3:4], col=hdbm$cluster+1, pch=20) # vengono solo due cluster ?????
#---------- HDBSCAN ---------#
# Fast implementation of the HDBSCAN (Hierarchical DBSCAN) and its related algorithms using Rcpp.
hdbm = hdbscan(iris[, 3:4], minPts=1000,     gen_simplified_tree = T)
#---------- HDBSCAN ---------#
# Fast implementation of the HDBSCAN (Hierarchical DBSCAN) and its related algorithms using Rcpp.
hdbm = hdbscan(iris[, 3:4], minPts=1,     gen_simplified_tree = T)
plot(iris[, 3:4], col=hdbm$cluster+1, pch=20) # vengono solo due cluster ?????
#---------- HDBSCAN ---------#
# Fast implementation of the HDBSCAN (Hierarchical DBSCAN) and its related algorithms using Rcpp.
hdbm = hdbscan(iris[, 3:4], minPts=2,     gen_simplified_tree = T)
plot(iris[, 3:4], col=hdbm$cluster+1, pch=20) # vengono solo due cluster ?????
rm(list=ls())
ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point()
#---------- K-MEANS ---------#
# Perform k-means clustering on a data matrix.
kmx = kmeans( iris[,3:4], 3, nstart=20)
table(kmx$cluster, iris$Species)
kmx$cluster <- as.factor(kmx$cluster)
ggplot(iris, aes(Petal.Length, Petal.Width, color=kmx$cluster)) + geom_point()
# Provides ggplot2-based elegant visualization of partitioning methods
fviz_cluster(object = kmx, data = iris[, 3:4], geom = "point",
# choose.vars = c("Sepal.Length", "Sepal.Width"), stand = FALSE,
ellipse.type = "norm") + theme_bw()
#---------- K-MEDOIDS ---------#
# Partitioning (clustering) of the data into k clusters “around medoids”, a more robust version of K-means.
pamx <- pam(iris[,3:4], 3)
pamx$cluster <- as.factor(pamx$cluster)
ggplot(iris, aes(Petal.Length, Petal.Width, color = pamx$cluster)) + geom_point()
fviz_cluster(pamx, geom = "point",
ellipse.type = "norm") + theme_bw()
#---------- GAUSSIAN MIXTURE MODELS ---------#
# Model-based clustering based on parameterized finite Gaussian mixture models.
# G == number of clusters
gmm <- Mclust(iris[,3:4], G = 3, modelNames = c("VVE"))
plot(gmm, what = "classification")
fviz_cluster(object = gmm, data = iris, geom = "point",
ellipse.type = "norm") + theme_bw()
# BIC for parameterized Gaussian mixture models fitted by EM algorithm initialized by model-based hierarchical clustering.
bic <- mclustBIC(iris[,3:4])
gmm <- Mclust(iris[,3:4], x = bic)
plot(gmm, what = "classification")
fviz_cluster(object = gmm, data = iris, geom = "point",
ellipse.type = "norm") + theme_bw()
#---------- DBSCAN ---------#
# Fast calculation of the k-nearest neighbor distances in a matrix of points.
kNNdistplot(iris[, 3:4], k=4) # k == number of nearest neighbors
abline(h=0.2,lty=2)
dbm = fpc::dbscan(iris[, 3:4], eps=0.2, MinPts=4)
fviz_cluster(dbm, iris[, 3:4], geom = "point")
#---------- HDBSCAN ---------#
# Fast implementation of the HDBSCAN (Hierarchical DBSCAN) and its related algorithms using Rcpp.
hdbm = hdbscan(iris[, 3:4], minPts=2,     gen_simplified_tree = T)
plot(iris[, 3:4], col=hdbm$cluster+1, pch=20) # vengono solo due cluster ?????
#---------- DBSCAN ---------#
# Fast calculation of the k-nearest neighbor distances in a matrix of points.
kNNdistplot(iris[, 3:4], k=4) # k == number of nearest neighbors
abline(h=0.2,lty=2)
dbm = fpc::dbscan(iris[, 3:4], eps=0.2, MinPts=4)
fviz_cluster(dbm, iris[, 3:4], geom = "point")
#---------- HDBSCAN ---------#
# Fast implementation of the HDBSCAN (Hierarchical DBSCAN) and its related algorithms using Rcpp.
hdbm = hdbscan(iris[, 3:4], minPts=2,     gen_simplified_tree = T)
plot(iris[, 3:4], col=hdbm$cluster+1, pch=20) # vengono solo due cluster ?????
#---------- DBSCAN ---------#
# Fast calculation of the k-nearest neighbor distances in a matrix of points.
kNNdistplot(iris[, 3:4], k=4) # k == number of nearest neighbors
abline(h=0.2,lty=2)
dbm = fpc::dbscan(iris[, 3:4], eps=0.2, MinPts=4)
fviz_cluster(dbm, iris[, 3:4], geom = "point")
#---------- HDBSCAN ---------#
# Fast implementation of the HDBSCAN (Hierarchical DBSCAN) and its related algorithms using Rcpp.
hdbm = hdbscan(iris[, 3:4], minPts=2,     gen_simplified_tree = T)
plot(iris[, 3:4], col=hdbm$cluster+1, pch=20) # vengono solo due cluster ?????
#---------- HDBSCAN ---------#
# Fast implementation of the HDBSCAN (Hierarchical DBSCAN) and its related algorithms using Rcpp.
hdbm = hdbscan(iris[, 3:4], minPts=10,     gen_simplified_tree = T)
plot(iris[, 3:4], col=hdbm$cluster+1, pch=20) # vengono solo due cluster ?????
#---------- HDBSCAN ---------#
# Fast implementation of the HDBSCAN (Hierarchical DBSCAN) and its related algorithms using Rcpp.
hdbm = hdbscan(iris[, 3:4], minPts=20,     gen_simplified_tree = T)
plot(iris[, 3:4], col=hdbm$cluster+1, pch=20) # vengono solo due cluster ?????
#---------- HDBSCAN ---------#
# Fast implementation of the HDBSCAN (Hierarchical DBSCAN) and its related algorithms using Rcpp.
hdbm = hdbscan(iris[, 3:4], minPts=5,     gen_simplified_tree = T)
plot(iris[, 3:4], col=hdbm$cluster+1, pch=20) # vengono solo due cluster ?????
