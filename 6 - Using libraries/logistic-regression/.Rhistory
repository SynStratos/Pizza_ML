for( i in 1:nrow(y)){
if(y[i,1]==1)   c[i] <- 0
if(y[i,2]==1)   c[i] <- 1
if(y[i,3]==1)   c[i] <- 2
}
c
}
multi <- all_classes(data[,11:13])
all_classes <- function(y){
c <- rep(1,nrow(y))
for( i in 1:nrow(y)){
if(y[i,1]==1)   c[i,] <- 0
if(y[i,2]==1)   c[i,] <- 1
if(y[i,3]==1)   c[i,] <- 2
}
c
}
multi <- all_classes(data[,11:13])
head(multi)
head(t(multi))
multi <- all_classes(data[,11:13])
all_classes <- function(y){
c <- rep(1,nrow(y))
for( i in 1:nrow(y)){
if(y[i,1]==1)   c[i,] <- 0
if(y[i,2]==1)   c[i,] <- 1
if(y[i,3]==1)   c[i,] <- 2
}
c
}
multi <- all_classes(data[,11:13])
# import data
data <- read.csv("candy_2.csv", header=FALSE)
colnames(data) <- c("caramel", "peanutyalmondy", "nougat", "crispedricewafer", "hard", "bar", "pluribus", "sugarpercent", "pricepercent", "winpercent", "choco", "fruit", "other")
all_classes <- function(y){
c <- rep(1,nrow(y))
for( i in 1:nrow(y)){
if(y[i,1]==1)   c[i,] <- 0
if(y[i,2]==1)   c[i,] <- 1
if(y[i,3]==1)   c[i,] <- 2
}
c
}
multi <- all_classes(data[,11:13])
all_classes <- function(y){
c <- rep(1,nrow(y))
for( i in 1:nrow(y)){
if(y[i,1]==1)   c[i] <- 0
if(y[i,2]==1)   c[i] <- 1
if(y[i,3]==1)   c[i] <- 2
}
c
}
multi <- all_classes(data[,11:13])
data1 <- cbind(data[,1:10], multi)
# min-max normalization
normalized <- lapply( data1[,1:10], function(x) round((x-min(x))/(max(x)-min(x)), 1))
data <- cbind(normalized, data1[11])
# train the model
my_formula<- as.formula(paste(names(training)[11], " ~ ", paste(names(training)[-11], collapse= "+")))
my_model <- glm( formula = my_formula,
data = training)
summary(my_model)
result <- predict(my_model, test, type="response")
print(result)
# train the model
my_formula<- as.formula(paste(names(training)[11], " ~ ", paste(names(training)[-11], collapse= "+")))
library(nnet)
my_model1 <- multinom( formula = my_formula,
data = training)
result <- predict(my_model, test)
print(result)
# my_model <- glm( formula = my_formula,
#                  data = training)
summary(my_model1)
result <- predict(my_model1, test)
print(result)
result <- predict(my_model1, test, type="prob")
print(result)
# confusion matrix
cm <- table(predict(my_model1), test[,11])
print(cm)
# confusion matrix
cm <- table(result, test[,11])
print(cm)
View(test)
View(test)
rm(list=ls())
# import data
data <- read.csv("candy_2.csv", header=FALSE)
colnames(data) <- c("caramel", "peanutyalmondy", "nougat", "crispedricewafer", "hard", "bar", "pluribus", "sugarpercent", "pricepercent", "winpercent", "choco", "fruit", "other")
all_classes <- function(y){
c <- rep(1,nrow(y))
for( i in 1:nrow(y)){
if(y[i,1]==1)   c[i] <- 0
if(y[i,2]==1)   c[i] <- 1
if(y[i,3]==1)   c[i] <- 2
}
c
}
multi <- all_classes(data[,11:13])
data1 <- cbind(data[,1:10], multi)
# min-max normalization
normalized <- lapply( data1[,1:10], function(x) round((x-min(x))/(max(x)-min(x)), 1))
data <- cbind(normalized, data1[11])
# split data
set.seed(222)
ind <- sample(2, nrow(data), replace=TRUE, prob= c(0.8,0.2))
training <- data[ind==1,]
test <- data[ind==2,]
# train the model
my_formula<- as.formula(paste(names(training)[11], " ~ ", paste(names(training)[-11], collapse= "+")))
library(nnet)
my_model1 <- multinom( formula = my_formula,
data = training)
# my_model <- glm( formula = my_formula,
#                  data = training)
summary(my_model1)
result <- predict(my_model1, test, type="prob")
print(result)
# confusion matrix
cm <- table(result, test[,11])
View(result)
View(result)
# my_model <- glm( formula = my_formula,
#                  data = training)
summary(my_model1)
result <- predict(my_model1, test)
print(result)
# confusion matrix
cm <- table(result, test[,11])
print(cm)
rm(list=ls())
# import data
data <- read.csv("candy_2.csv", header=FALSE)
colnames(data) <- c("caramel", "peanutyalmondy", "nougat", "crispedricewafer", "hard", "bar", "pluribus", "sugarpercent", "pricepercent", "winpercent", "choco", "fruit", "other")
# funzione per unire tutte le 3 classi in una singola colonna e assegnare un valore diverso per classe
all_classes <- function(y){
c <- rep(1,nrow(y))
for( i in 1:nrow(y)){
if(y[i,1]==1)   c[i] <- 0
if(y[i,2]==1)   c[i] <- 1
if(y[i,3]==1)   c[i] <- 2
}
c
}
multi <- all_classes(data[,11:13])
data <- cbind(data[,1:10], multi)
# min-max normalization
normalized <- lapply( data[,1:10], function(x) round((x-min(x))/(max(x)-min(x)), 1))
data <- cbind(normalized, data[11])
# split data
set.seed(222)
ind <- sample(2, nrow(data), replace=TRUE, prob= c(0.8,0.2))
training <- data[ind==1,]
test <- data[ind==2,]
# train the model
my_formula<- as.formula(paste(names(training)[11], " ~ ", paste(names(training)[-11], collapse= "+")))
library(nnet)
my_model1 <- multinom( formula = my_formula,
data = training)
# my_model <- glm( formula = my_formula,
#                  data = training)
summary(my_model1)
result <- predict(my_model1, test)
print(result)
# confusion matrix
cm <- table(result, test[,11])
print(cm)
print(cm)
error <- 1 - sum(diag(conf)) / sum(conf)
# confusion matrix
conf <- table(result, test[,11])
print(conf)
error <- 1 - sum(diag(conf)) / sum(conf)
print(error)
# test set esercitazione
sample_1 <- cbind(0,0,0,0,0,0,1,0.31299999,0.31299999,44.375519)
sample_2 <- cbind(1,0,0,0,1,0,0,0.186,0.26699999,41.904308)
sample_3 <- cbind(0,0,0,1,0,0,1,0.87199998,0.84799999,49.524113)
test_2 <- as.data.frame(rbind(sample_1, sample_2, sample_3))
colnames(test_2) <- c("caramel", "peanutyalmondy", "nougat", "crispedricewafer", "hard", "bar", "pluribus", "sugarpercent", "pricepercent", "winpercent")
# prediction su test set
result_2 <- predict(my_model1, test_2)
print(result_2)
cat("vaffanculo")
print("vaffanculo")
data <- read.csv("candy_1.csv", header=FALSE)
data <- read.csv("candy_2.csv", header=FALSE)
colnames(data) <- c("caramel", "peanutyalmondy", "nougat", "crispedricewafer", "hard", "bar", "pluribus", "sugarpercent",
"pricepercent", "winpercent", "choco", "fruit", "other")
choco_model <- glm( choco ~ data[1:10],
data)
choco_model <- glm( choco ~ data[1:10],
data,
family = "binomial")
choco_model <- glm( my_formula,
data)
my_formula <- as.formula(paste(names(data)[11], " ~ ", paste(names(data)[1:10], collapse= "+")))
choco_model <- glm( my_formula,
data)
choco_model <- glm( formula = my_formula,
data = data,
family = binomial(link="logit"))
result_1 <- predict(choco_model, test_2)
# test on new samples
sample_1 <- cbind(0,0,0,0,0,0,1,0.31299999,0.31299999,44.375519)
sample_2 <- cbind(1,0,0,0,1,0,0,0.186,0.26699999,41.904308)
sample_3 <- cbind(0,0,0,1,0,0,1,0.87199998,0.84799999,49.524113)
test_2 <- as.data.frame(rbind(sample_1, sample_2, sample_3))
colnames(test_2) <- c("caramel", "peanutyalmondy", "nougat", "crispedricewafer", "hard", "bar", "pluribus", "sugarpercent", "pricepercent", "winpercent")
result_1 <- predict(choco_model, test_2)
result_1 <- predict(choco_model, test_2)
result_1 <- lapply(result_1[1:3,], function(x) (which.max(x)))
my_formula <- as.formula(paste(names(data)[11], " ~ ", paste(names(data)[1:10], collapse= "+")))
choco_model <- glm( formula = my_formula,
data = data,
family = binomial(link="logit"))
my_formula <- as.formula(paste(names(data)[12], " ~ ", paste(names(data)[1:10], collapse= "+")))
fruit_model <- glm( formula = my_formula,
data = data,
family = binomial(link="logit"))
my_formula <- as.formula(paste(names(data)[13], " ~ ", paste(names(data)[1:10], collapse= "+")))
other_model <- glm( formula = my_formula,
data = data,
family = binomial(link="logit"))
# test on new samples
sample_1 <- cbind(0,0,0,0,0,0,1,0.31299999,0.31299999,44.375519)
sample_2 <- cbind(1,0,0,0,1,0,0,0.186,0.26699999,41.904308)
sample_3 <- cbind(0,0,0,1,0,0,1,0.87199998,0.84799999,49.524113)
test_2 <- as.data.frame(rbind(sample_1, sample_2, sample_3))
colnames(test_2) <- c("caramel", "peanutyalmondy", "nougat", "crispedricewafer", "hard", "bar", "pluribus", "sugarpercent", "pricepercent", "winpercent")
result_1 <- predict(choco_model, test_2)
result_2 <- predict(fruit_model, test_2)
result_3 <- predict(other_model, test_2)
result <- cbind(result_1, result_2, result_3)
View(result)
View(result)
result <- rbind(result_1, result_2, result_3)
result <- rbind(result_1, result_2, result_3)
result <- lapply(result[1:3,], function(x) (which.max(x)))
result <- lapply(result, function(x) (which.max(x)))
result <- lapply(result, function(x) (which.max(x)))
View(result)
# prediction su test set
result_2 <- predict(my_model, test_2, type="response")
result <- rbind(result_1, result_2, result_3)
source('~/Machine Learning/prove con "librerie"/logistic-regression/23_11.R', echo=TRUE)
result <- lapply(result, function(x) (which.max(x)))
View(result)
View(result)
# prediction su test set
result_2 <- predict(my_model, test_2, type="response")
#----------Regressione Logistica multi-classe----------#
rm(list=ls())
data <- read.csv("candy_2.csv", header=FALSE)
colnames(data) <- c("caramel", "peanutyalmondy", "nougat", "crispedricewafer", "hard", "bar", "pluribus", "sugarpercent",
"pricepercent", "winpercent", "choco", "fruit", "other")
my_formula <- as.formula(paste(names(data)[11], " ~ ", paste(names(data)[1:10], collapse= "+")))
choco_model <- glm( formula = my_formula,
data = data,
family = binomial(link="logit"))
my_formula <- as.formula(paste(names(data)[12], " ~ ", paste(names(data)[1:10], collapse= "+")))
fruit_model <- glm( formula = my_formula,
data = data,
family = binomial(link="logit"))
my_formula <- as.formula(paste(names(data)[13], " ~ ", paste(names(data)[1:10], collapse= "+")))
other_model <- glm( formula = my_formula,
data = data,
family = binomial(link="logit"))
# test on new samples
sample_1 <- cbind(0,0,0,0,0,0,1,0.31299999,0.31299999,44.375519)
sample_2 <- cbind(1,0,0,0,1,0,0,0.186,0.26699999,41.904308)
sample_3 <- cbind(0,0,0,1,0,0,1,0.87199998,0.84799999,49.524113)
test_2 <- as.data.frame(rbind(sample_1, sample_2, sample_3))
colnames(test_2) <- c("caramel", "peanutyalmondy", "nougat", "crispedricewafer", "hard", "bar", "pluribus", "sugarpercent", "pricepercent", "winpercent")
result_1 <- predict(choco_model, test_2)
result_2 <- predict(fruit_model, test_2)
result_3 <- predict(other_model, test_2)
result <- rbind(result_1, result_2, result_3)
resultasd <- lapply(result, function(x) (which.max(x)))
View(result)
View(result)
resultb <- max.col(result)
rm(list=ls())
# import data
data <- read.csv("candy_2.csv", header=FALSE)
colnames(data) <- c("caramel", "peanutyalmondy", "nougat", "crispedricewafer", "hard", "bar", "pluribus", "sugarpercent", "pricepercent", "winpercent", "choco", "fruit", "other")
# funzione per unire tutte le 3 classi in una singola colonna e assegnare un valore diverso per classe
all_classes <- function(y){
c <- rep(1,nrow(y))
for( i in 1:nrow(y)){
if(y[i,1]==1)   c[i] <- 0
if(y[i,2]==1)   c[i] <- 1
if(y[i,3]==1)   c[i] <- 2
}
c
}
multi <- all_classes(data[,11:13])
data <- cbind(data[,1:10], multi)
# min-max normalization
normalized <- lapply( data[,1:10], function(x) round((x-min(x))/(max(x)-min(x)), 1))
data <- cbind(normalized, data[11])
# split data
set.seed(222)
ind <- sample(2, nrow(data), replace=TRUE, prob= c(0.8,0.2))
training <- data[ind==1,]
test <- data[ind==2,]
# train the model
my_formula<- as.formula(paste(names(training)[11], " ~ ", paste(names(training)[-11], collapse= "+")))
library(nnet)
my_model1 <- multinom( formula = my_formula,
data = training)
# my_model <- glm( formula = my_formula,
#                  data = training)
summary(my_model1)
result <- predict(my_model1, test)
print(result)
# confusion matrix
conf <- table(result, test[,11])
print(conf)
error <- 1 - sum(diag(conf)) / sum(conf)
print(error)
# test set esercitazione
sample_1 <- cbind(0,0,0,0,0,0,1,0.31299999,0.31299999,44.375519)
sample_2 <- cbind(1,0,0,0,1,0,0,0.186,0.26699999,41.904308)
sample_3 <- cbind(0,0,0,1,0,0,1,0.87199998,0.84799999,49.524113)
test_2 <- as.data.frame(rbind(sample_1, sample_2, sample_3))
colnames(test_2) <- c("caramel", "peanutyalmondy", "nougat", "crispedricewafer", "hard", "bar", "pluribus", "sugarpercent", "pricepercent", "winpercent")
# prediction su test set
result_2 <- predict(my_model1, test_2)
print(result_2)
result <- cbind(result_1, result_2, result_3)
result_1 <- predict(choco_model, test_2)
result_2 <- predict(fruit_model, test_2)
result_3 <- predict(other_model, test_2)
#----------Regressione Logistica multi-classe----------#
rm(list=ls())
data <- read.csv("candy_2.csv", header=FALSE)
colnames(data) <- c("caramel", "peanutyalmondy", "nougat", "crispedricewafer", "hard", "bar", "pluribus", "sugarpercent",
"pricepercent", "winpercent", "choco", "fruit", "other")
my_formula <- as.formula(paste(names(data)[11], " ~ ", paste(names(data)[1:10], collapse= "+")))
choco_model <- glm( formula = my_formula,
data = data,
family = binomial(link="logit"))
my_formula <- as.formula(paste(names(data)[12], " ~ ", paste(names(data)[1:10], collapse= "+")))
fruit_model <- glm( formula = my_formula,
data = data,
family = binomial(link="logit"))
my_formula <- as.formula(paste(names(data)[13], " ~ ", paste(names(data)[1:10], collapse= "+")))
other_model <- glm( formula = my_formula,
data = data,
family = binomial(link="logit"))
# test on new samples
sample_1 <- cbind(0,0,0,0,0,0,1,0.31299999,0.31299999,44.375519)
sample_2 <- cbind(1,0,0,0,1,0,0,0.186,0.26699999,41.904308)
sample_3 <- cbind(0,0,0,1,0,0,1,0.87199998,0.84799999,49.524113)
test_2 <- as.data.frame(rbind(sample_1, sample_2, sample_3))
colnames(test_2) <- c("caramel", "peanutyalmondy", "nougat", "crispedricewafer", "hard", "bar", "pluribus", "sugarpercent", "pricepercent", "winpercent")
result_1 <- predict(choco_model, test_2)
result_2 <- predict(fruit_model, test_2)
result_3 <- predict(other_model, test_2)
result <- cbind(result_1, result_2, result_3)
resultb <- max.col(result)
resultasd <- lapply(result, function(x) (which.max(x)))
result_index <- lapply(result, function(x) (which.max(x)))
cat(result_index)
print(result_index)
result_index <- max.col(result)
print(result_index)
result <- cbind(result_1, result_2, result_3)
View(result)
View(result)
#----------Valutazione di un modello di classificazione----------#
rm(list=ls())
# import data
data <- read.csv("candy_1.csv", header=FALSE)
colnames(data) <- c("caramel", "peanutyalmondy", "nougat", "crispedricewafer", "hard", "bar", "pluribus", "sugarpercent", "pricepercent", "winpercent", "choco")
# z-score normalization
mu <- apply(data[-11], 2, mean)
std <- apply(data[-11], 2, sd)
data[-11] <- sweep(sweep(data[-11], 2L, mu), 2, std, "/")
# split 80/20
set.seed(222)
ind <- sample(2, nrow(data), replace=TRUE, prob= c(0.8,0.2))
training <- data[ind==1,]
test <- data[ind==2,]
#----------Valutazione di un modello di classificazione----------#
rm(list=ls())
# import data
data <- read.csv("candy_1.csv", header=FALSE)
colnames(data) <- c("caramel", "peanutyalmondy", "nougat", "crispedricewafer", "hard", "bar", "pluribus", "sugarpercent", "pricepercent", "winpercent", "choco")
# z-score normalization
mu <- apply(data[-11], 2, mean)
std <- apply(data[-11], 2, sd)
data[-11] <- sweep(sweep(data[-11], 2L, mu), 2, std, "/")
# split 80/20
set.seed(222)
ind <- sample(2, nrow(data), replace=TRUE, prob= c(0.8,0.2))
training <- data[ind==1,]
test <- data[ind==2,]
# train the model
my_formula <- as.formula(paste(names(training)[11], " ~ ", paste(names(training)[-11], collapse= "+")))
my_model <- glm( formula = my_formula,
data = training,
family = "binomial")
# prediction su test set
result <- predict(my_model, test, type="response")
# confusion matrix e misclassification error
predict <- ifelse(result>0.5, 1, 0)
conf <- table(Predicted = predict, Actual = test$choco)
error <- 1 - sum(diag(conf)) / sum(conf)
# precision
precision <- conf[1:1]/sum(conf[1,])
# recall
recall <- conf[1:1]/sum(conf[,1])
# f1-score
f1_score <- 2 * (precision * recall) / (precision + recall)
# test on new samples (23-11-2018 es.2)
sample_1 <- cbind(1, 0, 0, 0, 1, 0, 0, 0.186, 0.26699999, 41.904308)
sample_2 <- cbind(0, 0, 0, 1, 0, 0, 1, 0.87199998, 0.84799999, 49.524113)
test_2 <- as.data.frame(rbind(sample_1, sample_2))
colnames(test_2) <- c("caramel", "peanutyalmondy", "nougat", "crispedricewafer", "hard", "bar", "pluribus", "sugarpercent", "pricepercent", "winpercent")
# prediction su test set
result_2 <- predict(my_model, test_2, type="response")
predict_2 <- ifelse(result_2>0.5, 1, 0)
(
(
# import data
data <- read.csv("candy_1.csv", header=FALSE)
colnames(data) <- c("caramel", "peanutyalmondy", "nougat", "crispedricewafer", "hard", "bar", "pluribus", "sugarpercent", "pricepercent", "winpercent", "choco")
# z-score normalization
mu <- apply(data[-11], 2, mean)
std <- apply(data[-11], 2, sd)
data[-11] <- sweep(sweep(data[-11], 2L, mu), 2, std, "/")
# split 80/20
set.seed(222)
ind <- sample(2, nrow(data), replace=TRUE, prob= c(0.8,0.2))
#----------Valutazione di un modello di classificazione----------#
rm(list=ls())
# import data
data <- read.csv("candy_1.csv", header=FALSE)
colnames(data) <- c("caramel", "peanutyalmondy", "nougat", "crispedricewafer", "hard", "bar", "pluribus", "sugarpercent", "pricepercent", "winpercent", "choco")
# z-score normalization
mu <- apply(data[-11], 2, mean)
std <- apply(data[-11], 2, sd)
data[-11] <- sweep(sweep(data[-11], 2L, mu), 2, std, "/")
# split 80/20
set.seed(222)
ind <- sample(2, nrow(data), replace=TRUE, prob= c(0.8,0.2))
training <- data[ind==1,]
test <- data[ind==2,]
# train the model
my_formula <- as.formula(paste(names(training)[11], " ~ ", paste(names(training)[-11], collapse= "+")))
my_model <- glm( formula = my_formula,
data = training,
family = "binomial")
# prediction su test set
result <- predict(my_model, test, type="response")
# confusion matrix e misclassification error
predict <- ifelse(result>0.5, 1, 0)
conf <- table(Predicted = predict, Actual = test$choco)
error <- 1 - sum(diag(conf)) / sum(conf)
print(conf)
rm(list=ls())
# import data
data <- read.csv("candy_2.csv", header=FALSE)
colnames(data) <- c("caramel", "peanutyalmondy", "nougat", "crispedricewafer", "hard", "bar", "pluribus", "sugarpercent", "pricepercent", "winpercent", "choco", "fruit", "other")
# funzione per unire tutte le 3 classi in una singola colonna e assegnare un valore diverso per classe
all_classes <- function(y){
c <- rep(1,nrow(y))
for( i in 1:nrow(y)){
if(y[i,1]==1)   c[i] <- 0
if(y[i,2]==1)   c[i] <- 1
if(y[i,3]==1)   c[i] <- 2
}
c
}
multi <- all_classes(data[,11:13])
data <- cbind(data[,1:10], multi)
# min-max normalization
normalized <- lapply( data[,1:10], function(x) round((x-min(x))/(max(x)-min(x)), 1))
data <- cbind(normalized, data[11])
# split data
set.seed(222)
ind <- sample(2, nrow(data), replace=TRUE, prob= c(0.8,0.2))
training <- data[ind==1,]
test <- data[ind==2,]
# train the model
my_formula<- as.formula(paste(names(training)[11], " ~ ", paste(names(training)[-11], collapse= "+")))
library(nnet)
my_model1 <- multinom( formula = my_formula,
data = training)
# my_model <- glm( formula = my_formula,
#                  data = training)
summary(my_model1)
result <- predict(my_model1, test)
print(result)
# confusion matrix
conf <- table(result, test[,11])
print(conf)
error <- 1 - sum(diag(conf)) / sum(conf)
print(error)
# test set esercitazione
sample_1 <- cbind(0,0,0,0,0,0,1,0.31299999,0.31299999,44.375519)
sample_2 <- cbind(1,0,0,0,1,0,0,0.186,0.26699999,41.904308)
sample_3 <- cbind(0,0,0,1,0,0,1,0.87199998,0.84799999,49.524113)
test_2 <- as.data.frame(rbind(sample_1, sample_2, sample_3))
colnames(test_2) <- c("caramel", "peanutyalmondy", "nougat", "crispedricewafer", "hard", "bar", "pluribus", "sugarpercent", "pricepercent", "winpercent")
# prediction su test set
result_2 <- predict(my_model1, test_2)
print(result_2)
